{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dunkin assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports used throughout script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform, norm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import ultranest\n",
    "import pymc as pm\n",
    "import arviz as az\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_biscuit_type(dataframe, biscuits):\n",
    "    \"\"\"\n",
    "    Add new column to dataframe, changing biscuit names to integers. \n",
    "\n",
    "    Args:\n",
    "        dataframe (df): dataframe containing 'biscuit' column\n",
    "        biscuits (list): list of biscuit names in correct order\n",
    "    \"\"\"\n",
    "    \n",
    "    # check biscuit column exists\n",
    "    if 'biscuit' in dataframe.columns:\n",
    "        biscuit_column = dataframe['biscuit']\n",
    "    else:\n",
    "        raise ValueError(\"\\\"biscuit\\\" column does not exist in this dataframe.\")\n",
    "    \n",
    "    for i, biscuit in enumerate(biscuits):\n",
    "        biscuit_column = np.where([biscuit_column==biscuit], i, biscuit_column)[0]\n",
    "        \n",
    "    # set values to ints\n",
    "    dataframe['encoded biscuit'] = pd.to_numeric(biscuit_column)\n",
    "    \n",
    "    print(\"added encoded biscuit column\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def washburn(gamma,phi,eta,r,t):\n",
    "    \"\"\"\n",
    "    Function to calculate L using the washburn equation. \n",
    "\n",
    "    Args:\n",
    "        gamma (float): tea surface tension, in N m^(-1).\n",
    "        phi (float): contact angle between the biscuit and the tea surface, in rad.\n",
    "        eta (float):  tea dynamic viscosity, in Pa s\n",
    "        r (float): radius of the pore, in m\n",
    "        t (float): time after initial dunking that the measurement was made, in s.\n",
    "\n",
    "    Returns:\n",
    "        L (float): distance up the biscuit that the tea was visible, in m.\n",
    "    \"\"\"\n",
    "    \n",
    "    numerator = gamma*r*t*np.cos(phi)\n",
    "    denominator = 2*eta\n",
    "    \n",
    "    L = np.sqrt(numerator / denominator)\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pymc(data, gamma=6.78e-2, phi=1.45, eta=9.93e-4, rlow=1.5e-7, rhigh=1.2e-6):\n",
    "    \"\"\"\n",
    "    Determine value of pore radius required for Washburn model to fit the real data\n",
    "\n",
    "    Args:\n",
    "        data (df): time-resolved data that Washburn model will be fit to\n",
    "        gamma (float): tea surface tension. Defaults to 6.78e-2.\n",
    "        phi (float): contact angle between the biscuit and the tea surface. Defaults to 1.45.\n",
    "        eta (float):  tea dynamic viscosity. Defaults to 9.93e-4.\n",
    "        rlow (float): lower bound of possible radius values. Defaults to 1.5e-7.\n",
    "        rhigh (float): upper bound of possible radius values. Defaults to 1.2e-6.\n",
    "\n",
    "    Returns:\n",
    "        float: mean of determined distribution of pore radius. \n",
    "    \"\"\"\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        gamma=gamma\n",
    "        r = pm.Uniform('r', rlow, rhigh)\n",
    "        phi = phi\n",
    "        eta = eta\n",
    "        \n",
    "        L = pm.Normal('t', \n",
    "                    mu=washburn(gamma,phi,eta,r,data[\"t\"]), \n",
    "                    sigma=data['dL'], \n",
    "                    observed=data['L'])\n",
    "        \n",
    "        trace = pm.sample(1000, tune=1000, chains=10, progressbar=True)\n",
    "        \n",
    "\n",
    "    az.plot_trace(trace, var_names=[\"r\"])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return float(trace.posterior[\"r\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob_in_distribution(mean, std, value):\n",
    "    \"\"\"\n",
    "    Calculate probability of a given value to occur in a normal distribution\n",
    "\n",
    "    Args:\n",
    "        mean (float): Mean of normal distribution\n",
    "        std (float): Std of normal distribution\n",
    "        value (float): Value to have probability calculated \n",
    "    Returns:\n",
    "        percentage (float): Probability of value or a more extreme value occuring in the provided distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    if value > mean:\n",
    "        cum_prob = 1 - norm(mean, std).cdf(value)\n",
    "    elif value < mean:\n",
    "        cum_prob = norm(mean, std).cdf(value)\n",
    "        \n",
    "    # two-tailed test\n",
    "    # and converted to a percentage\n",
    "    percentage = 100*2*cum_prob\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_num_stds(mean, std, pymc_r):\n",
    "    \"\"\"\n",
    "    Calculate number of standard deviations a given value is from the mean of a normal distribution\n",
    "\n",
    "    Args:\n",
    "        mean (float): Mean of normal distribution\n",
    "        std (float): Std of normal distribution\n",
    "        value (float): Value to have probability calculated \n",
    "    Returns:\n",
    "        number_stds (float): number of standard deviations a given value is from the mean \n",
    "    \"\"\"\n",
    "    \n",
    "    number_stds = abs( pymc_r - mean) /std\n",
    "    \n",
    "    return number_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used for ultranest sampling. They require global variables, which are defined when called but cannot be input into the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(params, model):\n",
    "    \"\"\"\n",
    "    Likelihood function for a model with Gaussian errors.\n",
    "    \n",
    "    params (?): Parameters of the model.\n",
    "    model (func): Model to be used\n",
    "    \n",
    "    likelihood (float) The likelihood of the model given the data.\n",
    "    \"\"\"\n",
    "    model_y = model(tr_data['t'], params)\n",
    "    likelihood = np.sum([d.logpdf(m) for d, m in zip(data_distribution, model_y)])\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used for the original model\n",
    "\n",
    "def original_prior_transform(u):\n",
    "    \"\"\"\n",
    "    Transform uniform random variables to the model parameters.\n",
    "    \n",
    "    u (?): Uniform random variables\n",
    "    \n",
    "    transformed_variables (list): Model parameters\n",
    "    \"\"\"\n",
    "    transformed_variables = [p.ppf(u_) for p, u_ in zip(original_priors, u)]\n",
    "    return transformed_variables\n",
    "\n",
    "\n",
    "def original_model(t, params):\n",
    "    \"\"\"\n",
    "    Normal Washburn Relationship\n",
    "    \n",
    "    t (pd.Series): Time values\n",
    "    params (list): Parameters used in model\n",
    "    \n",
    "    L (arr): values for L\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t=t.values.reshape(-1,1)\n",
    "    except:\n",
    "        raise ValueError(\"t is not a pandas series\")\n",
    "    \n",
    "    r = biscuit_summaries[biscuit]['mean']\n",
    "    gamma=6.78e-2\n",
    "    phi=1.45\n",
    "    eta=9.93e-4 \n",
    "    \n",
    "    L = washburn(gamma,phi,eta,r,t)\n",
    "    \n",
    "    # L at time t.\n",
    "    return L\n",
    "\n",
    "def likelihood_original(params):\n",
    "    \"\"\"\n",
    "    Likelihood function for the original Washburn model.\n",
    "    \n",
    "    params (list?): Parameters used in model\n",
    "    \n",
    "    returns:\n",
    "        likelihood\n",
    "    \"\"\"\n",
    "    return likelihood(params, original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used for the original model\n",
    "\n",
    "def corrected_prior_transform(u):\n",
    "    \"\"\"\n",
    "    Transform uniform random variables to the model parameters.\n",
    "    \n",
    "    u (?): Uniform random variables\n",
    "    \n",
    "    transformed_variables (list): Model parameters\n",
    "    \"\"\"\n",
    "    return [p.ppf(u_) for p, u_ in zip(corrected_priors, u)]\n",
    "\n",
    "def corrected_model(t, params):\n",
    "    \"\"\"\n",
    "    Corrected Washburn Relationship\n",
    "    \n",
    "    t (pd.Series): Time values\n",
    "    params (list): Parameters used in model\n",
    "    \n",
    "    L_corrected (arr): values for corrected L\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t=t.values.reshape(-1,1)\n",
    "    except:\n",
    "        raise ValueError(\"t is not a pandas series/ pandas dataframe column\")\n",
    "    \n",
    "    # Correction factor!!\n",
    "    a = params[0]\n",
    "    \n",
    "    r =  biscuit_summaries[biscuit]['mean']\n",
    "    gamma=6.78e-2\n",
    "    phi=1.45\n",
    "    eta=9.93e-4 \n",
    "    \n",
    "    L = washburn(gamma,phi,eta,r,t)\n",
    "    L_corrected = (a)*L \n",
    "    \n",
    "    # L at time t.\n",
    "    return L_corrected\n",
    "\n",
    "\n",
    "def likelihood_corrected(params):\n",
    "    \"\"\"\n",
    "    The likelihood function for the corrected model.\n",
    "    \n",
    "    params (list?): Parameters used in model\n",
    "    \n",
    "    returns:\n",
    "        likelihood\n",
    "    \"\"\"\n",
    "    return likelihood(params, corrected_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big data collection\n",
    "\n",
    "The first provided dataset contains the following data for each individual biscuit: \n",
    "\n",
    "1. gamma: tea surface tension, in N m−1.\n",
    "2. phi: angle between the biscuit and the tea surface, in rad.\n",
    "3. eta: tea dynamic viscosity, in Pa s\n",
    "4. L: distance up the biscuit that the tea was absorbed, in m.\n",
    "5. t: time biscuit was dunked for before measurement done, in s.\n",
    "6. biscuit: type of biscuit that was dunked, out of Rich Tea, Hobnob or Digestive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dunking-data, and show dataframe\n",
    "\n",
    "big_data = pd.read_csv(\"dunking-data.csv\")\n",
    "print(f\"shape: {big_data.shape}\")\n",
    "big_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names of all types of biscuits\n",
    "\n",
    "biscuits = np.unique(big_data[\"biscuit\"])\n",
    "biscuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode names to integers\n",
    "\n",
    "encode_biscuit_type(big_data, biscuits)\n",
    "big_data['encoded biscuit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of biscuit type \n",
    "\n",
    "In this section, a ML classifier is trained and tested on the dunking-data.csv. It aims to be able to classify biscuit type from the provided data (phi,eta,L,t).\n",
    "\n",
    "GridSearchCV is used to determine the best hyperparameters for two models - random forest classifier, and support vector classification (SVC), and then use the better performing model throughout the rest of the script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(big_data, test_size=0.4)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "scaled_training_features = scaler.fit_transform(train.drop(['biscuit', 'encoded biscuit'], axis=1))\n",
    "scaled_test_features = scaler.fit_transform(test.drop(['biscuit', 'encoded biscuit'], axis=1))\n",
    "\n",
    "training_results = train['encoded biscuit']\n",
    "true_test_vals = test['encoded biscuit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GridSearchCV\n",
    "rfc_param_grid = [\n",
    "    {'n_estimators': [1000]},   # this value were determined as optimal using GridSearchCV - more trees worked better than fewer trees.\n",
    "  {'max_features': [ None]}     # they have been narrowed down in order to run faster. \n",
    " ]\n",
    "\n",
    "svc_param_grid = [\n",
    "  {'C': [5000, 1000, 100, 10, 5],  \n",
    "  'gamma': [0.001, 0.01, 0.1, 8, 'scale', 'auto'], \n",
    "  'kernel': ['rbf']} \n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model to use\n",
    "\n",
    "methods = {'Random Forest': [RandomForestClassifier(), rfc_param_grid], 'SVM': [SVC(), svc_param_grid] }\n",
    "\n",
    "current_f1 = 0\n",
    "for i, method in methods.items():\n",
    "    print(i)\n",
    "    \n",
    "    # Do gridsearchcv, and determine best estimator for each method\n",
    "    gscv = GridSearchCV(method[0], method[1], scoring=\"f1_weighted\")\n",
    "    gscv.fit(scaled_training_features, training_results)\n",
    "    print(f\"{i} BEST ESTIMATOR: {gscv.best_estimator_}\")\n",
    "    print(f'Estimator Top F1-Score: {gscv.best_score_:.3f}')\n",
    "    \n",
    "    # Only update model if the new model is better\n",
    "    if gscv.best_score_ > current_f1:\n",
    "        print(\"Updating best model\")\n",
    "        \n",
    "        best_estimator = gscv.best_estimator_\n",
    "        predicted_test_vals = best_estimator.predict(scaled_test_features)\n",
    "        \n",
    "        f1score = f1_score(true_test_vals, predicted_test_vals, average='weighted')\n",
    "        \n",
    "        print(f'True F1-Score: {f1score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen here that the SVC model consistently produces a much greater f1-score than the random forest model. The SVC model will be used throughout the rest of this analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare raw number of predicted biscuit types and true biscuit types\n",
    "\n",
    "max_num_predictions = 0\n",
    "for i, biscuit in enumerate(biscuits):\n",
    "    num_true_test_vals = len(true_test_vals[true_test_vals == i])\n",
    "    num_pred_test_vals = len(predicted_test_vals[predicted_test_vals == i])\n",
    "    \n",
    "    print( f\"Number of true {biscuit} values: {num_true_test_vals}\")\n",
    "    print( f\"Number of predicted {biscuit} values: {num_pred_test_vals}\")\n",
    "    \n",
    "    max_num_predictions = max(max_num_predictions, num_pred_test_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar chart\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "for i, biscuit in enumerate(biscuits):\n",
    "    # Get values where this biscuit was predicted\n",
    "    biscuit_prediction = np.where(predicted_test_vals==i, True, False)\n",
    "\n",
    "    # Filter test values to just these biscuit predictions\n",
    "    biscuit_test = true_test_vals[biscuit_prediction]\n",
    "    \n",
    "    # Number of correct predictions\n",
    "    correct_predictions = len( biscuit_test[biscuit_test==i] )\n",
    "    \n",
    "    ax.bar(biscuit, correct_predictions, color=f\"C{i}\", label=biscuit)\n",
    "    \n",
    "    base = correct_predictions\n",
    "    for j in range(len(biscuits)):\n",
    "        if j != i:\n",
    "            # NUmber of incorrect predictions, recording which biscuit was predicted instead\n",
    "            incorrect_predictions = len( biscuit_test[biscuit_test==j] )\n",
    "            ax.bar(biscuit, incorrect_predictions, bottom=base, color=f\"C{j}\")\n",
    "            base += incorrect_predictions\n",
    "            \n",
    "ax.set_ylabel(\"Number of predictions\")\n",
    "ax.set_ylim(0, max_num_predictions+5)\n",
    "ax.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "precision = precision_score(true_test_vals, predicted_test_vals, average='weighted')\n",
    "print(f'Precision: {precision:.3f}')\n",
    "\n",
    "recall = recall_score(true_test_vals, predicted_test_vals, average='weighted')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "\n",
    "f1score = f1_score(true_test_vals, predicted_test_vals, average='weighted')\n",
    "print(f'F1-Score: {f1score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microscopy Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next provided dataset was generated by taking a subset, one-sixth, of the dunking-data.csv dataset used in the analysis above. However, the biscuit type label has been dropped, and instead it includes the pore radius of the biscuits. This was measured using microscopy. \n",
    "\n",
    "Therefore, the columns of this dataset are:\n",
    "1. gamma: tea surface tension, in N m−1.\n",
    "2. phi: angle between the biscuit and the tea surface, in rad.\n",
    "3. eta: tea dynamic viscosity, in Pa s\n",
    "4. L: distance up the biscuit that the tea was absorbed, in m.\n",
    "5. t: time biscuit was dunked for before measurement done, in s.\n",
    "6. r: the radius of the biscuit pores, in m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset\n",
    "\n",
    "microscopy_data = pd.read_csv(\"microscopy-data.csv\")\n",
    "microscopy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions of biscuit type for each row. \n",
    "\n",
    "# scaled test data (drop the microscopy true r values)\n",
    "micro_model_features = microscopy_data.drop(['r'], axis=1)\n",
    "print(f\"{micro_model_features.columns}\")\n",
    "scaled_micro_features = scaler.fit_transform(micro_model_features)\n",
    "\n",
    "# Make predictions for biscuit type of each row using the best model determined above\n",
    "micro_predictions = best_estimator.predict(scaled_micro_features)\n",
    "len(micro_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine true distribution (combine the two datasets, and compare biscuit type and measured radius)\n",
    "\n",
    "training_merged = big_data.merge(microscopy_data, how='inner')\n",
    "true_distributions = training_merged[['encoded biscuit', 'r']].copy()\n",
    "true_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare distributions using the predicted and true values for biscuit type\n",
    "# plot radii distribution of each biscuit as histograms\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "biscuit_summaries = {}\n",
    "\n",
    "xmin = np.min(true_distributions[\"r\"])\n",
    "xmax = np.max(true_distributions[\"r\"])\n",
    "\n",
    "for i, biscuit in enumerate(biscuits):\n",
    "    print(f\"{biscuit}\")\n",
    "    \n",
    "    # get rows where encoded biscuit was predicted as i\n",
    "    map = np.where(micro_predictions==i, True, False)\n",
    "    \n",
    "    # get r values for these rows\n",
    "    biscuit_r_data = microscopy_data[\"r\"][map]\n",
    "    \n",
    "    # get true r values for these biscuits (using merged dataset)\n",
    "    true_biscuit_r_data = true_distributions[\"r\"][true_distributions['encoded biscuit']==i]\n",
    "    \n",
    "    # plot as 2 histograms\n",
    "    ax[0].hist(biscuit_r_data, bins=70, alpha=0.7, density=True, label=biscuit)\n",
    "    ax[1].hist(true_biscuit_r_data, bins=70, alpha=0.7, density=True, label=biscuit)\n",
    "    \n",
    "    # get mean and std of each distribution\n",
    "    mean, std = norm.fit(biscuit_r_data)\n",
    "    true_mean, true_std = norm.fit(true_biscuit_r_data)\n",
    "    \n",
    "    \n",
    "    x = np.linspace(0.9*xmin, 1.1*xmax, 100)\n",
    "    p = (norm.pdf(x, mean, std))\n",
    "    p_true = (norm.pdf(x, true_mean, true_std))\n",
    "    \n",
    "    ax[0].plot(x,p, color=f\"C{i}\")\n",
    "    ax[1].plot(x,p_true, color=f\"C{i}\")\n",
    "    \n",
    "    # add mean and std to biscuit dictionary\n",
    "    biscuit_summaries[biscuit] = {'mean': mean, 'std': std}\n",
    "    \n",
    "    # Print comparison between predicted and true distributions\n",
    "    print(f\"predicted mean: {mean:.3}\")\n",
    "    print(f\"true mean: {true_mean:.3}\")\n",
    "    print(f\"predicted std: {std:.3}\")\n",
    "    print(f\"true std: {true_std:.3}\\n\")\n",
    "    \n",
    "ax[0].set_xlabel(\"Pore radius size / m\")\n",
    "ax[0].set_ylabel(\"Probability Density\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Predicted Pore Radii Distributions\")\n",
    "ax[1].set_xlabel(\"Pore radius size / m\")\n",
    "ax[1].set_ylabel(\"Probability Density\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Known Pore Radii Distributions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows that the model to classify biscuits is effective, and produces highly accurate distributions. The biggest discrepancy is the increased value of the standard deviation for Rich Tea biscuits. However, this discrepancy is minimal.\n",
    "\n",
    "The distributions of the models show that Digestives have the largest pore radius, then Hobnobs, and Rich Tea biscuits have the smallest radii. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-resolved measurements\n",
    "Another dataset has been provided, which investigates the capillary flow rate of the tea in the\n",
    "biscuits. This involved taking a blind sample of each of the biscuits and measuring the length\n",
    "that the tea soaked up the biscuit in a time range from 30 s to 300 s.\n",
    "\n",
    "• the tea surface tension (gamma) was measured at 6.78 × 10−2 N m−1.\n",
    "\n",
    "• the contact angle (phi) was 1.45 rad.\n",
    "\n",
    "• the tea dynamic viscosity (eta) was 9.93 × 10−4 Pa s.\n",
    "\n",
    "The biscuit used for each measurement is unknown, and the data files are titled tr-1.csv, tr-2.csv, and tr-3.csv. Each data file contains the same information, three columns of experimental data:\n",
    "\n",
    "1. t: the time elapsed in the measurement, in s, the dependent variable.\n",
    "2. L: the length the tea has soaked up the biscuit, the independent variable, in m.\n",
    "3. dL: an estimate of the uncertainty in length, also in m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data plots\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(1,4):\n",
    "    data = pd.read_csv(f\"tr-{i}.csv\")\n",
    "    x = data[\"t\"]\n",
    "    y = data[\"L\"]\n",
    "    dy = data[\"dL\"]\n",
    "    \n",
    "    ax.errorbar(x,y,dy,label=f\"tr-{i}\")\n",
    "    \n",
    "ax.set_xlabel(\"t / s\")\n",
    "ax.set_ylabel(\"L / m\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign time-resolved data to respective biscuit, in order from largest pore radius (Digestive) to smallest pore radius (Rich Tea)\n",
    "\n",
    "tr_biscuits = [3,1,2]\n",
    "for i, tr_val in enumerate(tr_biscuits):\n",
    "    biscuit_summaries[biscuits[i]]['tr_vals'] = pd.read_csv(f\"tr-{tr_val}.csv\")\n",
    "    \n",
    "# show example of biscuit summary dictionary for one biscuit\n",
    "biscuit_summaries['Digestive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we test the effectiveness of the Washburn model. Each dataset has now been assigned to a biscuit. The pore radius of each biscuit used was not measured, so the average pore radius of each biscuit type (determined above) is used in each of the following models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# washburn model fittings using average pore radii for each biscuit\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Values measured for this specific experiment\n",
    "gamma = 6.78e-2\n",
    "phi = 1.45\n",
    "eta = 9.93e-4\n",
    "\n",
    "for i, biscuit in enumerate(biscuits):\n",
    "    data = biscuit_summaries[biscuit]['tr_vals']\n",
    "    t = data[\"t\"]\n",
    "    L = data[\"L\"]\n",
    "    dL = data[\"dL\"]\n",
    "    \n",
    "    r = biscuit_summaries[biscuit]['mean']\n",
    "    \n",
    "    washburn_L_preds = washburn(gamma, phi, eta, r, t)\n",
    "    ax.errorbar(t,L,dL,label=f\"{biscuit} True data\", alpha=0.7, color=f\"C{i}\")\n",
    "    \n",
    "    ax.plot(t, washburn_L_preds, marker='+', linestyle=\"\", color=f\"C{i}\", zorder=3, label=f'{biscuit} Washburn relationship') \n",
    "    \n",
    "ax.set_xlabel(\"Time / s\")\n",
    "ax.set_ylabel(\"Length of Tea Up Biscuit / m\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows the Washburn relationship is likely an inaccurate model, as it does not seem to fit the Digestive or Rich Tea data effectively. However, there is a possibility the Digestive and Rich Tea biscuits used in these measurements had abnormally large/small pore sizes respectively. The following analysis determines the radii of the biscuits required for the Washburn model to accurately fit the real data. This will then be compared to the pore radius biscuit_summaries, to determine the likelihood these pore radii are accurate, or if the model is inaccurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find r for washburn to fit. \n",
    "\n",
    "for biscuit in biscuits:\n",
    "    data = biscuit_summaries[biscuit]['tr_vals']\n",
    "    \n",
    "    pymc_r = run_pymc(data)\n",
    "    \n",
    "    print(f\"Predicted pore radius: {pymc_r}\")\n",
    "    \n",
    "    # add to summary dictionary\n",
    "    biscuit_summaries[biscuit]['pymc_r'] = pymc_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required radius for the digestive comes out at over 1.0e-6, which is significantly greater than the mean. The following analysis determines the likelihood of each biscuit having the radius required for the Washburn model to fit effectively, using the particle size distributions determined further up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for biscuit in biscuits:\n",
    "    print(biscuit)\n",
    "    \n",
    "    biscuit_dict = biscuit_summaries[biscuit]\n",
    "    \n",
    "    mean = biscuit_dict['mean']\n",
    "    std = biscuit_dict['std'] \n",
    "    pymc_r = biscuit_dict['pymc_r']\n",
    "    \n",
    "    \n",
    "    # Calculate probability of this radius occuring\n",
    "    percentage_prob = calc_prob_in_distribution(mean, std, pymc_r)\n",
    "    print(f\"Probability of this value or a more extreme value occuring: {percentage_prob:.2f} %\")\n",
    "    \n",
    "    # Calculate number of standard devisations of value from mean\n",
    "    num_stds = calc_num_stds(mean, std, pymc_r)\n",
    "    print(f\"Number of stds from mean: {num_stds:.3}\\n\")\n",
    "    \n",
    "    biscuit_summaries[biscuit]['value_prob'] = percentage_prob\n",
    "    biscuit_summaries[biscuit]['num_stds'] = num_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put data into csv file to be used in report\n",
    "\n",
    "table_columns = ['Mean', 'Std', 'Required value', 'Probability of required value occuring', 'Number of stds value is from mean' ]\n",
    "key_names = ['mean', 'std', 'pymc_r', 'value_prob', 'num_stds']\n",
    "print(table_columns)\n",
    "\n",
    "table1 = pd.DataFrame(columns=table_columns)\n",
    "for biscuit in biscuits:\n",
    "    for i, table_column in enumerate(table_columns):\n",
    "        table1.loc[biscuit,table_column] = biscuit_summaries[biscuit][key_names[i]]\n",
    "    \n",
    "table1.to_csv('probability_table.csv', index_label='Biscuit Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats showing probability of Washburn relationship mis-fitting due to an incorrect radius size estimate\n",
    "\n",
    "table1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, it is clear that the Washburn equation is not a good prediction model. The probability of the Washburn model being correct for Digestives is extremely low, as the chance of the required radius occuring is less than 0.06%. This suggests there is instead something wrong with the model. \n",
    "\n",
    "The Washburn model effectively predicts the relationship between L and t for Hobnobs. Furthermore, the model for the Rich Tea biscuit is likely correct, as the required radius for the probability is relatively likely to occur. However, the predicted model for the Digestives is very unlikely, which suggests the model can and should be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An improved model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, a correction factor is added to the Washburn relationship. This corrected model is compared to the original model for each biscuit type, to see if the corrected model signifcantly improves upon the original model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine an improved model using a correction factor. \n",
    "\n",
    "# record log(B) for later\n",
    "log_Bs = []\n",
    "\n",
    "# priors of each model\n",
    "original_priors = []  \n",
    "corrected_priors = [uniform(0, 10)]\n",
    "\n",
    "for biscuit in biscuits:\n",
    "    tr_data = biscuit_summaries[biscuit]['tr_vals']\n",
    "\n",
    "    data_distribution = [norm(loc=loc, scale=scale) for loc, scale in zip(tr_data['L'], tr_data['dL'])]\n",
    "\n",
    "    # run original model\n",
    "    sampler_one = ultranest.ReactiveNestedSampler([], likelihood_original, original_prior_transform)\n",
    "    sampler_one.run(show_status=False)\n",
    "    sampler_one.print_results()\n",
    "\n",
    "    # run corrected model\n",
    "    sampler_two = ultranest.ReactiveNestedSampler(['a'], likelihood_corrected, corrected_prior_transform)\n",
    "    sampler_two.run(show_status=False)\n",
    "    sampler_two.print_results()\n",
    "    \n",
    "    # add correction to summary stats\n",
    "    biscuit_summaries[biscuit]['correction'] = sampler_two.results['posterior']['mean'][0]\n",
    "    \n",
    "    # record log(B)\n",
    "    log_B = sampler_two.results['logz'] - sampler_one.results['logz']\n",
    "    print(f\"log_B: {log_B:.3}\")\n",
    "    \n",
    "    log_Bs.append(log_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected washburn model fittings\n",
    "\n",
    "table2 = pd.DataFrame(columns=(['log_B', 'Original model MSE', 'Corrected model MSE']))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "for i, biscuit in enumerate(biscuits):\n",
    "    data = biscuit_summaries[biscuit]['tr_vals']\n",
    "    t = data[\"t\"]\n",
    "    L = data[\"L\"]\n",
    "    dL = data[\"dL\"]\n",
    "    \n",
    "    # calculate \n",
    "    washburn_L_preds = washburn(gamma, phi, eta, biscuit_summaries[biscuit]['mean'], t )\n",
    "    ax.errorbar(t,L,dL,label=f\"{biscuit} True data\", alpha=0.4, color=f\"C{i}\", zorder=1)\n",
    "    \n",
    "    ax.plot(t, washburn_L_preds, marker='+', linestyle='', color=f\"C{i}\", zorder=3, label=f'{biscuit} Washburn relationship') \n",
    "    \n",
    "    correction = biscuit_summaries[biscuit]['correction']\n",
    "    if i == len(biscuits)-1:\n",
    "        label = 'Corrected Washburn relationships'\n",
    "    else:\n",
    "        label=None\n",
    "        \n",
    "    corrected_model = (correction)*washburn_L_preds\n",
    "    ax.plot(t, corrected_model, linestyle='--', color=\"black\", zorder=5, label=label) \n",
    "    \n",
    "    mse_original = mean_squared_error(L, washburn_L_preds)\n",
    "    mse_corrected = mean_squared_error(L, corrected_model)\n",
    "    \n",
    "    table2.loc[biscuit] = [log_Bs[i], mse_original, mse_corrected]\n",
    "    \n",
    "ax.set_xlabel(\"Time / s\")\n",
    "ax.set_ylabel(\"Length of Tea Up Biscuit / m\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "table2.to_csv('model_comparisons.csv', index_label='Biscuit Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison statistics between the two models.\n",
    "\n",
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between corrections and (estimated) pore radius.\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "radii_mean = np.zeros(len(biscuits))\n",
    "corrections = np.zeros(len(biscuits))\n",
    "for i, biscuit in enumerate(biscuits):\n",
    "    radii_mean[i] = (biscuit_summaries[biscuit]['mean'])\n",
    "    corrections[i] = (biscuit_summaries[biscuit]['correction'])\n",
    "    \n",
    "ax.scatter(radii_mean, corrections, marker='x')\n",
    "\n",
    "ax.set_xlabel('Radius / m')\n",
    "ax.set_ylabel('Correction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These corrections look like they may have a linear relationship, which would produce a neat wrapper for relating the correction function to readius. The following code determines the relationship and plots it. However, it should be considered that this relationship is very unreliable as there are only three data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find linear relationship\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "x = radii_mean.reshape(-1, 1)\n",
    "y = corrections.reshape(-1, 1)\n",
    "\n",
    "model.fit(x,y)\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot linear relationship\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "radii_mean = np.zeros(len(biscuits))\n",
    "corrections = np.zeros(len(biscuits))\n",
    "\n",
    "for i, biscuit in enumerate(biscuits):\n",
    "    radii_mean[i] = (biscuit_summaries[biscuit]['mean'])\n",
    "    corrections[i] = (biscuit_summaries[biscuit]['correction'])\n",
    "    \n",
    "ax.scatter(radii_mean, corrections, marker='x')\n",
    "\n",
    "# Plot linear regression model\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = model.intercept_[0] + model.coef_[0][0] * x_vals\n",
    "print(f\"Relationship: y = {model.intercept_[0]:.3} + {model.coef_[0][0]:.3} * r\")\n",
    "ax.plot(x_vals, y_vals, linestyle='--', color='black')\n",
    "\n",
    "ax.set_xlabel('Radius / m')\n",
    "ax.set_ylabel('Correction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate r**2 value of this linear regression model\n",
    "\n",
    "predictions = model.predict(radii_mean.reshape(-1,1))\n",
    "\n",
    "r2 = r2_score(predictions, corrections)\n",
    "\n",
    "print(f\"r2 value: {r2:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure and R^2 value suggests there may be a linear relationship between the correction factor and the pore radius of a biscuit. However, with only 3 data points, this relationship is very unreliable. This unreliability is enhanced by the fact that the true pore radii of each biscuit used in this analysis was not known. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The corresponding report will outline how the data produced can be used in future data-driven investigations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics_w7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
